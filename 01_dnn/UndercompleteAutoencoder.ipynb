{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this assingnment, we will be using Pandas dataframes\n",
    "# to read, transform, and store MNIST images. Make sure\n",
    "# you are familiar with its API:\n",
    "# https://pandas.pydata.org/pandas-docs/stable/\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter here the Dataset ID that you have received in the spreadsheet.\n",
    "# Make sure that you use the ID that was assigned to you, \n",
    "# otherwise your submission will be graded against the wrong images.\n",
    "dataset_id = '20190927-233244_3a66bfb023d40470edbcf174694d0595'\n",
    "\n",
    "# Assignment datasets are stored in an AWS S3 bucket. The following code\n",
    "# downloads your dataset (~300MB) directly into RAM. Optionally,\n",
    "# you can save the datasets to the local disk, but that's really not required.\n",
    "prefix = f'https://ucla-deeplearning.s3-us-west-1.amazonaws.com/storage/mnist-0.2/jobs/transform_repack/{dataset_id}'\n",
    "\n",
    "# These three dataframes contain clean images. Use each dataframe\n",
    "# to train, validate, and test your model, respectively.\n",
    "train_clean_X = pd.read_parquet(f'{prefix}/train_clean_X.parquet')\n",
    "validate_clean_X = pd.read_parquet(f'{prefix}/validate_clean_X.parquet')\n",
    "test_clean_X = pd.read_parquet(f'{prefix}/test_clean_X.parquet')\n",
    "\n",
    "# These dataframes are noisy versions of the dataframes above.\n",
    "train_noisy_X = pd.read_parquet(f'{prefix}/train_noisy_X.parquet')\n",
    "validate_noisy_X = pd.read_parquet(f'{prefix}/validate_noisy_X.parquet')\n",
    "test_noisy_X = pd.read_parquet(f'{prefix}/test_noisy_X.parquet')\n",
    "\n",
    "# This is the segment of the dataset that will be graded in this assignment.\n",
    "# Your model has to denoise this dataframe, and you have to submit the denoised version of this dataframe.\n",
    "score_noisy_X = pd.read_parquet(f'{prefix}/score_noisy_X.parquet')\n",
    "\n",
    "# For additional context, these are labels (e.g. digits) corresponding\n",
    "# to each image in the dataset. Use it to analyze which digits\n",
    "# your models has more or less problems with. These dataframes\n",
    "# should not be used as part of the training process itself,\n",
    "# only for post-training analysis, if you wish so.\n",
    "train_y = pd.read_parquet(f'{prefix}/train_y.parquet')\n",
    "validate_y = pd.read_parquet(f'{prefix}/validate_y.parquet')\n",
    "test_y = pd.read_parquet(f'{prefix}/test_y.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just as in the lecture slides, the input matrix\n",
    "# has row vectors that hold pixels of a single 28x28 image.\n",
    "# Note that the column vectors are individual locations on\n",
    "# the 28x28 grid, but flattened in a row-major way (28x28=784).\n",
    "train_clean_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While not strictly required for this assignment,\n",
    "# you will probably want to plot images to\n",
    "# have a visual understanding of the model's performance.\n",
    "# You can use any Python plotting library, for example:\n",
    "# https://matplotlib.org/contents.html\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's select a clean image from the dataset\n",
    "image_pixels = train_clean_X.iloc[1]\n",
    "\n",
    "# In order to plot an image, you need to reshape\n",
    "# the flattened array back into a 28x28 grid.\n",
    "image_pixels = image_pixels.values.reshape(28, 28)\n",
    "\n",
    "plt.imshow(image_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's select a noisy image from the dataset\n",
    "image_pixels = train_noisy_X.iloc[1]\n",
    "\n",
    "# In order to plot an image, you need to reshape\n",
    "# the array into a 28x28 grid.\n",
    "image_pixels = image_pixels.values.reshape(28, 28)\n",
    "\n",
    "plt.imshow(image_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this couse, all assignments are expected to\n",
    "# implemented with Tensorflow framework.\n",
    "# For this task, we can use its high-level Keras API:\n",
    "# https://keras.io\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# This is an example of an undercomplete autoencoder.\n",
    "# Note that your autoencoder has to be either sparse, denoising, or both.\n",
    "# In other words, you will probably need to tweak these lines of code.\n",
    "layers = [\n",
    "    # Encoder: 50 nodes, Exponential Linear Unit activation\n",
    "    Dense(units=50, activation='elu', input_shape=(28 * 28,)),\n",
    "    \n",
    "    # Decoder: matches the shape of the image\n",
    "    # Sigmoid activation is needed because pixel\n",
    "    # values are real numbers between 0 (black) and 1 (white).\n",
    "    Dense(units=28*28, activation='sigmoid')\n",
    "]\n",
    "\n",
    "# Sequential class is a Keras class that attaches every \n",
    "# layer's output to the next layer's input.\n",
    "# This is essentially a helper class for any feedforward networks.\n",
    "autoencoder = Sequential(layers)\n",
    "\n",
    "# Using MSE as the loss function, and Adam optimizer.\n",
    "# Other optimizers and losses (e.g. binary cross entropy) can also be used.\n",
    "autoencoder.compile(\n",
    "    optimizer=optimizers.Adam(\n",
    "        lr=0.01\n",
    "    ), \n",
    "    loss='mean_squared_error'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to have reproducible results, we need\n",
    "# to set the seed values for NumPy and Tensorflow.\n",
    "# Keep these lines in the same cell as the training code,\n",
    "# so that you don't accidentally train multiples models\n",
    "# without reseting the seed.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "np.random.seed(11)\n",
    "tf.random.set_random_seed(11)\n",
    "\n",
    "# If an autoencoder is undercomplete, it can be trained in a reconstruction task.\n",
    "# Note that your autoencoder has to be sparse, denoising, or both.\n",
    "# In other words, you will probably need to tweak these lines of code.\n",
    "history = autoencoder.fit(\n",
    "    x=train_clean_X, \n",
    "    y=train_clean_X, \n",
    "    \n",
    "    # It's a good idea to have low number of epochs,\n",
    "    # to test the learning process, but not for the final model training.\n",
    "    epochs=3,\n",
    "    \n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    validation_data=(validate_clean_X, validate_clean_X)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can feed the noisy images into the autoencoder.\n",
    "train_denoised_X = autoencoder.predict(train_noisy_X)\n",
    "\n",
    "# Keras returns back a Numpy array, but we need Pandas dataframe,\n",
    "# so let's quickly put the denoised images into a dataframe.\n",
    "train_denoised_X = pd.DataFrame(train_denoised_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's confirm that the denoiser works.\n",
    "image_pixels = train_denoised_X.iloc[1]\n",
    "\n",
    "# In order to plot an image, you need to reshape\n",
    "# the array into a 28x28 grid.\n",
    "image_pixels = image_pixels.values.reshape(28, 28)\n",
    "\n",
    "plt.imshow(image_pixels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While you can calculate MSE metric yourself or ask Keras,\n",
    "# another way to do it is using scikit-learn:\n",
    "# https://scikit-learn.org/stable/\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# This calcualates MSE on train clean against train denoised,\n",
    "# which measures how well the autoencoder denoises the training dataset.\n",
    "# Note that you really want to calculate this metric on validate and/or test dataset.\n",
    "# Don't let an overfitting model fool you into using it for the graded submission!\n",
    "mean_squared_error(train_denoised_X, train_clean_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you are ready to make the graded submission,\n",
    "# run the autoencoder on the score noisy dataset.\n",
    "score_denoised_X = pd.DataFrame(\n",
    "    autoencoder.predict(score_noisy_X),\n",
    "    # This is needed to save the file in Parquet format.\n",
    "    columns=score_noisy_X.columns\n",
    ")\n",
    "\n",
    "# Now save it to disc as a Parquet file.\n",
    "score_denoised_X.to_parquet('score_denoised_X.parquet')\n",
    "\n",
    "# Next, let's save the model's definition.\n",
    "import json\n",
    "with open(f'keras_model.json', 'w') as f:\n",
    "    f.write(json.dumps(json.loads(autoencoder.to_json()), indent=True))\n",
    "\n",
    "# Finally, let's save the learned parameters.\n",
    "autoencoder.save_weights(f\"keras_parameters.h5\")\n",
    "\n",
    "# You now have the following files to be uploaded to Moodle:\n",
    "# 1. This notebook and any other Python code you used to train the final model.\n",
    "# 2. keras_model.json -- the model's definition\n",
    "# 3. keras_parameters.json -- the model's trained parameters\n",
    "# 4. score_denoised_X.parquet - the model's output on the score dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
