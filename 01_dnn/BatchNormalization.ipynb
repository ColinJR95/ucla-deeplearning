{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "As Standard Scaling improves learning by adjusting each feature in the input layer, Batch Normalization adjusts every unit of a layer.\n",
    "\n",
    "Estimated distribution is over the observations in the mini-batch.\n",
    "Typically, pre-activation value $Z$ is used, but this is not a hard rule.\n",
    "\n",
    "$$ \\mu = \\frac{1}{m} \\sum{z^{(i)}} $$\n",
    "\n",
    "$$ \\sigma^2 = \\frac{1}{m} \\sum{(z_i - \\mu)^2} $$\n",
    "\n",
    "$$ z^{(i)}_{norm} = \\frac{z^{(i)} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} $$\n",
    "\n",
    "$$ z^{(i)}_{final} = \\gamma z^{(i)}_{norm} + \\beta $$\n",
    "\n",
    "Parameters $\\gamma$ and $\\beta$ are learnable with back propagation.\n",
    "Batch normalization of layer L's pre-activation makes the input to the layer L+1 vary less, since the between-iterations, same-unit distribution's mean and variance are fixed. This improves the speed of learning.\n",
    "\n",
    "Since estimates of mean and variance are different for each batch, $z^{[l]}$ gets noisy, which also acts as small regularizer. The quantity of noise reduces with the size of the batch.\n",
    "\n",
    "For validation time, estimate $\\mu$ and $\\sigma^2$ as exponentially weighted average across the training mini-batches. Use these estimates instead of the validation dataset's properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcenter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbeta_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'zeros'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgamma_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ones'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmoving_mean_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'zeros'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmoving_variance_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ones'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbeta_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgamma_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbeta_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgamma_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrenorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrenorm_clipping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrenorm_momentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfused\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvirtual_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0madjustment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Batch normalization layer (Ioffe and Szegedy, 2014).\n",
       "\n",
       "Normalize the activations of the previous layer at each batch,\n",
       "i.e. applies a transformation that maintains the mean activation\n",
       "close to 0 and the activation standard deviation close to 1.\n",
       "\n",
       "Arguments:\n",
       "  axis: Integer, the axis that should be normalized\n",
       "      (typically the features axis).\n",
       "      For instance, after a `Conv2D` layer with\n",
       "      `data_format=\"channels_first\"`,\n",
       "      set `axis=1` in `BatchNormalization`.\n",
       "  momentum: Momentum for the moving average.\n",
       "  epsilon: Small float added to variance to avoid dividing by zero.\n",
       "  center: If True, add offset of `beta` to normalized tensor.\n",
       "      If False, `beta` is ignored.\n",
       "  scale: If True, multiply by `gamma`.\n",
       "      If False, `gamma` is not used.\n",
       "      When the next layer is linear (also e.g. `nn.relu`),\n",
       "      this can be disabled since the scaling\n",
       "      will be done by the next layer.\n",
       "  beta_initializer: Initializer for the beta weight.\n",
       "  gamma_initializer: Initializer for the gamma weight.\n",
       "  moving_mean_initializer: Initializer for the moving mean.\n",
       "  moving_variance_initializer: Initializer for the moving variance.\n",
       "  beta_regularizer: Optional regularizer for the beta weight.\n",
       "  gamma_regularizer: Optional regularizer for the gamma weight.\n",
       "  beta_constraint: Optional constraint for the beta weight.\n",
       "  gamma_constraint: Optional constraint for the gamma weight.\n",
       "  renorm: Whether to use Batch Renormalization\n",
       "    (https://arxiv.org/abs/1702.03275). This adds extra variables during\n",
       "    training. The inference is the same for either value of this parameter.\n",
       "  renorm_clipping: A dictionary that may map keys 'rmax', 'rmin', 'dmax' to\n",
       "    scalar `Tensors` used to clip the renorm correction. The correction\n",
       "    `(r, d)` is used as `corrected_value = normalized_value * r + d`, with\n",
       "    `r` clipped to [rmin, rmax], and `d` to [-dmax, dmax]. Missing rmax, rmin,\n",
       "    dmax are set to inf, 0, inf, respectively.\n",
       "  renorm_momentum: Momentum used to update the moving means and standard\n",
       "    deviations with renorm. Unlike `momentum`, this affects training\n",
       "    and should be neither too small (which would add noise) nor too large\n",
       "    (which would give stale estimates). Note that `momentum` is still applied\n",
       "    to get the means and variances for inference.\n",
       "  fused: if `None` or `True`, use a faster, fused implementation if possible.\n",
       "    If `False`, use the system recommended implementation.\n",
       "  trainable: Boolean, if `True` also add variables to the graph collection\n",
       "    `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).\n",
       "  virtual_batch_size: An `int`. By default, `virtual_batch_size` is `None`,\n",
       "    which means batch normalization is performed across the whole batch. When\n",
       "    `virtual_batch_size` is not `None`, instead perform \"Ghost Batch\n",
       "    Normalization\", which creates virtual sub-batches which are each\n",
       "    normalized separately (with shared gamma, beta, and moving statistics).\n",
       "    Must divide the actual batch size during execution.\n",
       "  adjustment: A function taking the `Tensor` containing the (dynamic) shape of\n",
       "    the input tensor and returning a pair (scale, bias) to apply to the\n",
       "    normalized values (before gamma and beta), only during training. For\n",
       "    example, if axis==-1,\n",
       "      `adjustment = lambda shape: (\n",
       "        tf.random_uniform(shape[-1:], 0.93, 1.07),\n",
       "        tf.random_uniform(shape[-1:], -0.1, 0.1))`\n",
       "    will scale the normalized value by up to 7% up or down, then shift the\n",
       "    result by up to 0.1 (with independent scaling and bias for each feature\n",
       "    but shared across all examples), and finally apply gamma and/or beta. If\n",
       "    `None`, no adjustment is applied. Cannot be specified if\n",
       "    virtual_batch_size is specified.\n",
       "\n",
       "Input shape:\n",
       "    Arbitrary. Use the keyword argument `input_shape`\n",
       "    (tuple of integers, does not include the samples axis)\n",
       "    when using this layer as the first layer in a model.\n",
       "\n",
       "Output shape:\n",
       "    Same shape as input.\n",
       "\n",
       "References:\n",
       "    - [Batch Normalization: Accelerating Deep Network Training by Reducing\n",
       "      Internal Covariate Shift](https://arxiv.org/abs/1502.03167)\n",
       "\u001b[0;31mFile:\u001b[0m           /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/layers/normalization.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     BatchNormalization\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow.keras.layers\n",
    "?tensorflow.keras.layers.BatchNormalization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
