{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "\n",
    "At each iteration, we are going to disable randomly selected neurons, then do both forward and backward pass\n",
    "with those neurons disabled. Numerically, we can sample a vector from Bernoulli distribution,\n",
    "then multiply the neuron's activation matrix's with this vector column-wise (e.g. disabling the same neuron across all samples).\n",
    "\n",
    "$$ p = \\text{probability of keeping a node} $$\n",
    "$$ \\mathbf{d} = \\text{vector of random variables ~ Bernoulli}(p)$$\n",
    "\n",
    "Now we can zero-out randomly selected columns of the activation matrix:\n",
    "$$ \n",
    "\\tilde{\\mathbf{a}}_{:, m}^{[1]} \n",
    "= \\frac {\\mathbf{a}_{:, m}^{[1]} * d_m} {p} \n",
    "$$\n",
    "\n",
    "Why divide by $p$? To keep the mean across the features constant:\n",
    "\n",
    "$$ \n",
    "\\mathrm{E}[\\tilde{\\mathbf{a}}_{s, :}^{[1]}]\n",
    "= \\frac {\\mathrm{E}[\\mathbf{a}_{s, :}^{[1]}] * \\mathrm{E}[\\mathbf{d}]} {p} \n",
    "= \\frac {\\mathrm{E}[\\mathbf{a}_{s, :}^{[1]}] * p}{p} \n",
    "= \\mathrm{E}[\\mathbf{a}_{s, :}^{[1]}]\n",
    "$$\n",
    "\n",
    "Why does this work as a regularization method? \n",
    "Intuitively, every node has to use information in all of its input nodes to minimize the impact of any particular input node being disabled. \n",
    "Numerically, this makes the L2-norm of the weights vector smaller.\n",
    "Theoretically, itâ€™s also similar to having an ensemble of neural networks, since each sampling of the dropout mask represents a different network.\n",
    "Overall, the dropout introduces noise robustness to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Applies Dropout to the input.\n",
       "\n",
       "Dropout consists in randomly setting\n",
       "a fraction `rate` of input units to 0 at each update during training time,\n",
       "which helps prevent overfitting.\n",
       "\n",
       "Arguments:\n",
       "    rate: float between 0 and 1. Fraction of the input units to drop.\n",
       "    noise_shape: 1D integer tensor representing the shape of the\n",
       "        binary dropout mask that will be multiplied with the input.\n",
       "        For instance, if your inputs have shape\n",
       "        `(batch_size, timesteps, features)` and\n",
       "        you want the dropout mask to be the same for all timesteps,\n",
       "        you can use `noise_shape=(batch_size, 1, features)`.\n",
       "    seed: A Python integer to use as random seed.\n",
       "\u001b[0;31mFile:\u001b[0m           /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     SpatialDropout1D, SpatialDropout2D, SpatialDropout3D, Dropout\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "krom tensorflow.keras.layers import Dropout\n",
    "?Dropout"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
